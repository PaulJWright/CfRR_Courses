{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65439eb-6056-49df-8ee7-e487c997fb66",
   "metadata": {},
   "source": [
    "# Collective MPI communication\n",
    "\n",
    "We've now covered how to use the MPI communicator to get information about a programs parallel topology and send data between processes. This fundamental functionality is enough to begin creating scalable, parallel programs, but there is more MPI functionality that can make manipulating large sets of data more dynamic, simple and powerful.\n",
    "Let's start as usual, by opening a new Python script (collectivate_comms.py) and importing mpi4py:\n",
    "\n",
    "```python\n",
    "# collective_comms.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "\n",
    "## MPI broadcasting\n",
    "\n",
    "The MPI broadcast method offers a simple and effective way to send a variable from one rank to all other ranks. Let's try it out:\n",
    "```python\n",
    "if comm.Get_rank() == 0:\n",
    "    msg = \"Broadcasted message hello\"\n",
    "else:\n",
    "    msg = None\n",
    "\n",
    "msg = comm.bcast(msg, root=0)\n",
    "print(f'Message \"{msg}\" recieved by rank {comm.Get_rank()}')\n",
    "```\n",
    "\n",
    "The simplicity of this method is great, but its utility is fairly limited as there aren't many instances where you need all ranks to have data that you can't simply hard-code. One such instance is reading small amounts of data (such as configuration files) where you can't simultaneously access the file system from all processes.\n",
    "\n",
    "## MPI scatter and gather\n",
    "\n",
    "MPI parallelism shines when you need to work on large amounts of data by enabling a program to pass a section of that data to a process and letting that process handle its own portion of the data. This approach is known as *decomposition*, and is ubiquitous across simulations that make use of MPI such as weather and ocean modelling, CFD, astrophysics, etc.\n",
    "Decomposition can be achieved in a number of ways, but the simplest way is to use the MPI *scatter* routine. Let's give it a try!\n",
    "We can add a `comm.barrier()` after the broadcast example we just completed and set up our scatter example. First, we need to create an example data set to scatter on the root rank. Lets create an array of random numbers, one for each rank:\n",
    "```python\n",
    "if comm.Get_rank() == 0:\n",
    "    rng = np.random.default_rng()\n",
    "    data = rng.integers(low=0, high=10, size=comm.Get_size())\n",
    "    print(f\"Rank 0 data is {data}\")\n",
    "else:\n",
    "    data = None\n",
    "```\n",
    "\n",
    "Now, to scatter that data from the root rank, we simply use the function:\n",
    "```python\n",
    "data = comm.scatter(data, root=0)\n",
    "```\n",
    "\n",
    "and each rank can print the data it recieved, to demonstrate that the process worked:\n",
    "```python\n",
    "print(f\"Rank {comm.Get_rank()} recieved data entry: {data}\")\n",
    "```\n",
    "\n",
    "Let's run our script with `mpirun -n 4 python collective_comms.py` and see what we get.\n",
    "\n",
    "We can see that our array has been split up into its component parts and scattered across each rank. Let's add another `comm.barrier()` and use the `gather` method to reverse the process:\n",
    "\n",
    "```python\n",
    "data = comm.gather(data, root=0)\n",
    "\n",
    "print(f\"Post gather: Rank {comm.Get_rank()} has data: {data}\")\n",
    "```\n",
    "\n",
    "We can see that the data has been re-collected back onto the root rank. Note that the data is no longer a numpy array but can trivially be converted back into one.\n",
    "\n",
    "## Large/non-uniform scatter and gather\n",
    "We can take the example above and apply it to a larger set of data. Lets create a sequential numpy array to demonstrate how the data is being broken up:\n",
    "\n",
    "```python\n",
    "if comm.Get_rank() == 0:\n",
    "    data = np.arange(200)\n",
    "    print(f\"Rank 0 data is {data}\")\n",
    "    data = np.array_split(data, comm.Get_size())\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.scatter(data, root=0)\n",
    "\n",
    "print(f\"Rank {comm.Get_rank()} recieved data entry: {data}\")\n",
    "```\n",
    "\n",
    "The `np.array_split` method is particularly useful here, as it allows us to reliably split a numpy array into equal-ish parts without need to solve that particular problem ourselves.\n",
    "When we run this code with MPI we can see that there is a predictable pattern to how the MPI scatter method will distribute the data in the array, which we can leverage to ensure that each set of data we scatter remains coherent on each of the destination processes.\n",
    "If we then use the same code as in our previous example to `gather` the data:\n",
    "```python\n",
    "data = comm.gather(data, root=0)\n",
    "\n",
    "print(f\"Post gather: Rank {comm.Get_rank()} has data: {data}\")\n",
    "```\n",
    "we can see that the resulting data is all present and in the same order as before, but split into several numpy arrays. We can fix this issue with `np.concatenate```:\n",
    "```python\n",
    "if not data is None:\n",
    "    data = np.concatenate(data)\n",
    "```\n",
    "\n",
    "## Global MPI operations\n",
    "\n",
    "For distributed memory problems, its difficult to get a holistic view of your entire data set as it doesnt exist in any one place. This means that performing global operations such as calculating the sum or product of a distributed data set also requires MPI. Fortunately, MPI has several functions that make this easier. Lets create a large set of data and scatter it across our processes, as before:\n",
    "\n",
    "```python\n",
    "if comm.Get_rank() == 0:\n",
    "    data = np.arange(1, 101, 5)\n",
    "    print(f\"Sum of rank 0 data = {np.sum(data)}\")\n",
    "    data = np.array_split(data, comm.Get_size())\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.scatter(data, root=0)\n",
    "```\n",
    "\n",
    "Now, we can use the `comm.reduce` operation to calculate the global sum:\n",
    "\n",
    "```python\n",
    "glob_sum = comm.reduce(data, MPI.SUM, root=0)\n",
    "\n",
    "if comm.Rank() == 0:\n",
    "    print(f\"MPI global sum = {np.sum(glob_sum)}\")\n",
    "```\n",
    "We can also compute the global product in a similar way:\n",
    "```python\n",
    "glob_prod = comm.reduce(data, MPI.PROD, root=0)\n",
    "\n",
    "if comm.Get_rank() == 0:\n",
    "    print(f\"MPI global product of data is {np.prod(glob_prod)}\")\n",
    "```\n",
    "\n",
    "Here is the result:\n",
    "```\n",
    "$ mpirun -n 4 python collective_comms.py\n",
    "...\n",
    "Sum of rank 0 data is 970\n",
    "Product of rank 0 data is 4168026667654053888\n",
    "MPI global sum of data is 970\n",
    "MPI global product of data is 4168026667654053888\n",
    "```\n",
    "\n",
    "There are some other MPI reduce operations available, but for anything sophisticated you will need to add additional layers of calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7fbe1-75db-4006-9eea-0af45e57a56f",
   "metadata": {},
   "source": [
    "# Complete File\n",
    "[Download complete collective_comms.py file](complete_files/collective_comms_complete.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a01652-4080-41cf-9d15-09de216b346d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
